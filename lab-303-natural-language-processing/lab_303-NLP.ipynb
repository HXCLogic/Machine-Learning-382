{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2cc3ec",
   "metadata": {},
   "source": [
    "----\n",
    "### Key Takeaways\n",
    "By the end of this module, you will gain a comprehensive understanding of sentiment analysis techniques, be able to preprocess text data effectively, build and evaluate sentiment analysis models using both traditional machine learning algorithms and deep learning techniques, and apply these skills to real-world NLP tasks.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f25262",
   "metadata": {},
   "source": [
    "### Introduction to NLP\n",
    "- Understanding Natural Language Processing (NLP) and its applications.\n",
    "- Overview of NLP techniques for text analysis and understanding.\n",
    "- Importance of sentiment analysis in understanding customer opinions and feedback.\n",
    "- -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d45d7",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "- __Tokenization:__ Breaking down text into smaller units such as words or sentences.\n",
    "- __Stopword Removal:__ Removing common words (e.g., \"the\", \"is\", \"and\") that do not carry significant meaning.\n",
    "- __Lemmatization:__ Reducing words to their base or root form (e.g., \"running\" to \"run\", \"better\" to \"good\").\n",
    "- ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e5384",
   "metadata": {},
   "source": [
    "#### Bag of Words Model\n",
    "\n",
    "- Introduction to the Bag of Words (BoW) model.\n",
    "- Creating a vocabulary of unique words from the text corpus.\n",
    "- Representing text documents as numerical vectors based on word frequency.\n",
    "- -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674adf2",
   "metadata": {},
   "source": [
    "\n",
    "#### Naive Bayes Classifier\n",
    "\n",
    "- Understanding the Naive Bayes algorithm for text classification.\n",
    "- Training a Naive Bayes classifier using labeled data for sentiment analysis.\n",
    "- Evaluating the classifier performance using accuracy, precision, recall, and F1-score.\n",
    "-------\n",
    "#### LSTM (Long Short-Term Memory) Model\n",
    "\n",
    "- Introduction to Recurrent Neural Networks (RNNs) and LSTM architecture.\n",
    "- Preprocessing text data for LSTM input.\n",
    "- Building and training an LSTM model for sentiment analysis.\n",
    "- Fine-tuning the model and optimizing hyperparameters.\n",
    "- Evaluating the LSTM model performance and comparing it with traditional methods.\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0b6a6",
   "metadata": {},
   "source": [
    "### Practical Applications and Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521649bf",
   "metadata": {},
   "source": [
    "- Sentiment analysis on customer reviews for products or services.\n",
    "- Analyzing social media sentiments for brand perception.\n",
    "- Understanding public opinions on political or social issues through text analysis.\n",
    "- -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556430d",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15f88ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=Warning, lineno=0, append=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bea3c",
   "metadata": {},
   "source": [
    "### Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3945fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text\n",
    "text_data = [\n",
    "    \"Wow... Loved this place.\",\n",
    "    \"Crust is not good.\",\n",
    "    \"Not tasty and the texture was just nasty.\",\n",
    "    \"Stopped by during the late May bank holiday of...\",\n",
    "    \"The selection on the menu was great and so wer...\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6efe13",
   "metadata": {},
   "source": [
    "#### 1. Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f586d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow... loved this place.',\n",
       " 'crust is not good.',\n",
       " 'not tasty and the texture was just nasty.',\n",
       " 'stopped by during the late may bank holiday of...',\n",
       " 'the selection on the menu was great and so wer...']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lower = [text.lower() for text in text_data]\n",
    "text_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ba8a5",
   "metadata": {},
   "source": [
    "#### 2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6ce7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_punc = [text.translate(str.maketrans('', '', string.punctuation)) for text in text_lower]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1758ab9",
   "metadata": {},
   "source": [
    "#### 3. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0fcd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'loved', 'this', 'place'],\n",
       " ['crust', 'is', 'not', 'good'],\n",
       " ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty'],\n",
       " ['stopped', 'by', 'during', 'the', 'late', 'may', 'bank', 'holiday', 'of'],\n",
       " ['the', 'selection', 'on', 'the', 'menu', 'was', 'great', 'and', 'so', 'wer']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokenized_data = [nltk.word_tokenize(text) for text in text_punc]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73099a06",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c71f941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "english_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fa4a6",
   "metadata": {},
   "source": [
    "#### 4. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b697350b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'loved', 'place'],\n",
       " ['crust', 'good'],\n",
       " ['tasty', 'texture', 'nasty'],\n",
       " ['stopped', 'late', 'may', 'bank', 'holiday'],\n",
       " ['selection', 'menu', 'great', 'wer']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "filtered_data = [[word for word in text if word not in english_stopwords] for text in tokenized_data]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3d202",
   "metadata": {},
   "source": [
    "#### 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04b70962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'are', 'running', 'on', 'the', 'road']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'cats are running on the roads'\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each token\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ee48a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'loved', 'place'],\n",
       " ['crust', 'good'],\n",
       " ['tasty', 'texture', 'nasty'],\n",
       " ['stopped', 'late', 'may', 'bank', 'holiday'],\n",
       " ['selection', 'menu', 'great', 'wer']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Lemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatized_text = [[lm.lemmatize(word) for word in text] for text in filtered_data]\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936768f7",
   "metadata": {},
   "source": [
    "#### 6. Stemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9b7ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', 'love', 'place'],\n",
       " ['crust', 'good'],\n",
       " ['tasti', 'textur', 'nasti'],\n",
       " ['stop', 'late', 'may', 'bank', 'holiday'],\n",
       " ['select', 'menu', 'great', 'wer']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_text = [[stemmer.stem(word) for word in text] for text in lemmatized_text]\n",
    "stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddcf50",
   "metadata": {},
   "source": [
    "#### Create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78c4552b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bank',\n",
       " 'crust',\n",
       " 'good',\n",
       " 'great',\n",
       " 'holiday',\n",
       " 'late',\n",
       " 'love',\n",
       " 'may',\n",
       " 'menu',\n",
       " 'nasti',\n",
       " 'place',\n",
       " 'select',\n",
       " 'stop',\n",
       " 'tasti',\n",
       " 'textur',\n",
       " 'wer',\n",
       " 'wow'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vocabulary\n",
    "vocabulary = set([word for text in stemmed_text for word in text])\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8bfd5",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2395a5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2703cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a781087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        wow loved place\n",
       "1                                             crust good\n",
       "2                                    tasty texture nasty\n",
       "3      stopped late may bank holiday rick steve recom...\n",
       "4                            selection menu great prices\n",
       "                             ...                        \n",
       "995                    think food flavor texture lacking\n",
       "996                              appetite instantly gone\n",
       "997                      overall impressed would go back\n",
       "998    whole experience underwhelming think well go n...\n",
       "999    hadnt wasted enough life poured salt wound dra...\n",
       "Name: Review, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "data_series_preprocessed = df['Review'].apply(preprocess_text)\n",
    "data_series_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e90b5f",
   "metadata": {},
   "source": [
    "*Polarity Score* - value ranging from -1 to 1, but also depends on the analysis tool or method used. Common ranges include: -1 to 1, -100 to 100, or 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45c78fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.8271\n",
       "1      0.4404\n",
       "2     -0.5574\n",
       "3      0.6908\n",
       "4      0.6249\n",
       "        ...  \n",
       "995    0.0000\n",
       "996    0.0000\n",
       "997    0.4767\n",
       "998    0.2732\n",
       "999    0.3875\n",
       "Name: Review, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiments = data_series_preprocessed.apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0efab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify sentiment as positive or negative based on compound score\n",
    "sentiment_class = sentiments.apply(lambda x: 'positive' if x > 0 else ('neutral' if x == 0 else 'negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c5a4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review Sentiment\n",
       "0                             Wow... Loved this place.  positive\n",
       "1                                   Crust is not good.  positive\n",
       "2            Not tasty and the texture was just nasty.  negative\n",
       "3    Stopped by during the late May bank holiday of...  positive\n",
       "4    The selection on the menu was great and so wer...  positive\n",
       "..                                                 ...       ...\n",
       "995  I think food should have flavor and texture an...   neutral\n",
       "996                           Appetite instantly gone.   neutral\n",
       "997  Overall I was not impressed and would not go b...  positive\n",
       "998  The whole experience was underwhelming, and I ...  positive\n",
       "999  Then, as if I hadn't wasted enough of my life ...  positive\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentiment scores to the DataFrame\n",
    "data_series_with_sentiment = pd.DataFrame({'Review': df['Review'], 'Sentiment': sentiment_class})\n",
    "\n",
    "# Display the DataFrame with sentiment scores\n",
    "data_series_with_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d0ba284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d4ebe",
   "metadata": {},
   "source": [
    "### Naives Bayes Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cee99f",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09237445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Liked\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "..                                                 ...    ...\n",
       "995  I think food should have flavor and texture an...      0\n",
       "996                           Appetite instantly gone.      0\n",
       "997  Overall I was not impressed and would not go b...      0\n",
       "998  The whole experience was underwhelming, and I ...      0\n",
       "999  Then, as if I hadn't wasted enough of my life ...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c574fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  all_stopwords.remove('not')\n",
    "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbe835aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "\n",
    "# Separate features from labels\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d729677",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ceed6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d6a44",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3284e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "457f9df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model\n",
    "model = (\n",
    "    BernoulliNB()\n",
    "    )\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e29078",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "263e5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {acc_score*100:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8093e90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on new dataset\n",
    "new_text = ['perfectly fine for me']\n",
    "\n",
    "new_vector = cv.transform(new_text)\n",
    "prediction = model.predict(new_vector)\n",
    "\n",
    "print(f'Sentiment: {\"Positive\" if prediction > 0 else \"Negative\"}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
